{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57db3093-cf30-496d-a6f5-c196c8b5ea8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "1c4d3559-1c89-4684-95c4-0b592cb6e570",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e004b972-f80e-4e3d-8d6b-b29d0ab5e050",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer_en = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer_si = AutoTokenizer.from_pretrained(\"Ransaka/sinhala-bert-medium-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f658f9d-78fc-4936-a381-98732f039d52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_en = \"This is english text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "363b5d34-2509-476f-b123-f08c1f9146db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoded_using_en = tokenizer_en.encode(text_en,add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d250e77-ed30-44d4-bd98-6b35a5321435",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoded_using_sin = tokenizer_si.encode(text_en,add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c41c8d2d-175b-48be-bb33-eb06d9f548e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f97c3ff-2acc-4316-ab1b-434119350247",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_using_sin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17849304-96d2-4e5c-9f24-d5b5d426693b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_using_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2592e856-914d-41ce-90da-d853e4bf71ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sinhala_text = \"සෞඛ්‍ය වැඩවර්ජනය අද (02) දිනයේත් අඛණ්ඩව ක්‍රියාත්මක කරන බව වෘත්තීය සමිති පවසයි.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d18518b-7b51-410d-9a39-9613aff92917",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "english_encode = tokenizer_en.encode(sinhala_text,add_special_tokens=False)\n",
    "sinhala_encode = tokenizer_si.encode(sinhala_text,add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "119e0f66-a545-4214-816c-f79b69ec54c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'( 02 ).'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_en.decode(english_encode,skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92411167-1e95-41f8-8701-2b519f2698f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'සෞඛ්ය වැඩවර්ජනය අද ( 02 ) දිනයේත් අඛණ්ඩව ක්රියාත්මක කරන බව වෘත්තීය සමිති පවසයි.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_si.decode(sinhala_encode,skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8b8568f-2737-46df-9607-2d5035c9e493",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_sinhala(text):\n",
    "    english_encoding = [enc for enc in tokenizer_en(text=text, add_special_tokens=False).input_ids if enc not in tokenizer_en.all_special_ids]\n",
    "    sinhala_encoding = [enc for enc in tokenizer_si(text=text, add_special_tokens=False).input_ids if enc not in tokenizer_si.all_special_ids]\n",
    "    print(english_encoding)\n",
    "    print(sinhala_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9607ed7d-df79-4560-924b-1bcc3d055196",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1006, 6185, 1007, 1012]\n",
      "[7017, 24289, 5753, 12, 9001, 13, 7791, 5618, 13642, 6584, 5737, 5672, 8442, 9470, 7372, 18]\n"
     ]
    }
   ],
   "source": [
    "is_sinhala(sinhala_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fe3d858a-32bc-44ee-954d-14965cf355e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_eocode = u\"සෞඛ්ය\".encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d06f7e76-8482-4593-81c8-170ab90b5472",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_eocode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "06404094-a727-46c6-a54f-3eb08342846d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xe0\\xb7\\x83\\xe0\\xb7\\x9e\\xe0\\xb6\\x9b\\xe0\\xb7\\x8a\\xe0\\xb6\\xba'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_eocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "90a45026-51fc-43ae-ae26-1445166cb14f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0xe0\n",
      "0xb7\n",
      "0x83\n",
      "0xe0\n",
      "0xb7\n",
      "0x9e\n",
      "0xe0\n",
      "0xb6\n",
      "0x9b\n",
      "0xe0\n",
      "0xb7\n",
      "0x8a\n",
      "0xe0\n",
      "0xb6\n",
      "0xba\n"
     ]
    }
   ],
   "source": [
    "for byte in text_eocode:\n",
    "    print(hex(byte))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f76308d9-ed53-431e-bab8-fecc5b515eef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0xe0'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b\"0xe0\".decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b71fe604-ee41-4c8e-8a7c-26e04fdc2109",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sinling import utils\n",
    "\n",
    "BASE_CONSONANTS = [\n",
    "    'ක', 'ඛ', 'ග', 'ඝ', 'ඞ', 'ඟ',\n",
    "    'ච', 'ඡ', 'ජ', 'ඣ', 'ඤ', 'ඦ',\n",
    "    'ට', 'ඨ', 'ඩ', 'ඪ', 'ණ', 'ඬ',\n",
    "    'ත', 'ථ', 'ද', 'ධ', 'න', 'ඳ',\n",
    "    'ප', 'ඵ', 'බ', 'භ', 'ම', 'ඹ',\n",
    "    'ය', 'ර', 'ල', 'ව',\n",
    "    'ශ', 'ෂ', 'ස', 'හ', 'ළ', 'ෆ',\n",
    "]\n",
    "\n",
    "SAN = [\n",
    "    'ඟ', 'ඦ', 'ඬ', 'ඳ', 'ඹ'\n",
    "]\n",
    "# TODO: Check whether these are correct mappings\n",
    "# ඟ = ඬ්ග, ඦ = ඤ්ජ, ඬ = ණ්ඩ, ඳ = න්ද, ඹ = ම්බ\n",
    "# Resource: http://www.danuma.lk/sinhala/index.php?option=com_content&view=article&id=19234%3A2010-06-09-10-17-10&\n",
    "# catid=110%3Aeducation&Itemid=76&lang=si\n",
    "SAN_MAPPING = {'ඟ': 'ංග', 'ඦ': 'ඤ්ජ', 'ඬ': 'ණ්ඩ', 'ඳ': 'න්ද', 'ඹ': 'ම්බ'}\n",
    "REVERSE_SAN_MAPPING = {d: v for v, d in SAN_MAPPING.items()}\n",
    "\n",
    "CONSONANTS = [c + '්' for c in BASE_CONSONANTS]\n",
    "\n",
    "VOWELS = [\n",
    "    'අ', 'ආ', 'ඇ', 'ඈ', 'ඉ', 'ඊ', 'උ', 'ඌ',\n",
    "    'ඍ', 'ඎ', 'එ', 'ඒ', 'ඓ', 'ඔ', 'ඕ', 'ඖ',\n",
    "    'අං', 'අඃ',\n",
    "]\n",
    "\n",
    "VOWEL_DIACRITICS = [\n",
    "    '', 'ා', 'ැ', 'ෑ', 'ි', 'ී', 'ු', 'ූ', 'ෘ',\n",
    "    'ෲ', 'ෙ', 'ේ', 'ෛ', 'ො', 'ෝ', 'ෞ',\n",
    "    'ං', 'ඃ', '්'\n",
    "]\n",
    "\n",
    "LONG_TO_SHORT_VOWEL_DIACRITICS_MAPPING = {\n",
    "    '': 'ා',\n",
    "    'ෑ': 'ැ',\n",
    "    'ී': 'ි',\n",
    "    'ූ': 'ු',\n",
    "    'ේ': 'ෙ',\n",
    "    'ෝ': 'ො'\n",
    "}\n",
    "\n",
    "DIACRITICS_MAPPING = {v: d for v, d in zip(VOWELS, VOWEL_DIACRITICS)}\n",
    "\n",
    "REVERSE_DIACRITICS_MAPPING = {d: v for v, d in zip(VOWELS, VOWEL_DIACRITICS)}\n",
    "\n",
    "CONJUNCT_CONSONANTS = [\n",
    "    'ක්ර', 'ඛ්ර', 'ග්ර', 'ඝ්ර', 'ඞ්ර', 'ඟ්ර',\n",
    "    'ක්ය', 'ඛ්ය', 'ග්ය', 'ඝ්ය', 'ඞ්ය', 'ඟ්ය',\n",
    "    'ක්ෂ', '෴',\n",
    "]\n",
    "\n",
    "NUMERALS = [\n",
    "    '𑇡', '𑇢', '𑇣', '𑇤', '𑇥', '𑇦', '𑇧', '𑇨', '𑇩', '𑇪',\n",
    "    '𑇫', '𑇬', '𑇭', '𑇮', '𑇯', '𑇰', '𑇱', '𑇲', '𑇳', '𑇴',\n",
    "]\n",
    "\n",
    "# COMBINED_LETTERS = CONSONANTS + utils.combine(BASE_CONSONANTS, VOWEL_DIACRITICS)\n",
    "\n",
    "# COMBINED_SAN = SAN + utils.combine(SAN, VOWEL_DIACRITICS)\n",
    "\n",
    "GOSHA_LETTERS = [\n",
    "    'අ', 'ආ', 'ඇ', 'ඈ', 'ඉ', 'ඊ', 'උ', 'ඌ',\n",
    "    'ඍ', 'ඎ', 'එ', 'ඒ', 'ඓ', 'ඔ', 'ඕ', 'ඖ',\n",
    "    'අං', 'අඃ',\n",
    "    'ග', 'ඝ', 'ඞ',\n",
    "    'ජ', 'ඣ', 'ඤ',\n",
    "    'ඩ', 'ඪ', 'ණ',\n",
    "    'ද', 'ධ', 'න',\n",
    "    'බ', 'භ', 'ම',\n",
    "    'ය', 'ර', 'ල', 'ව',\n",
    "    'හ'\n",
    "]\n",
    "\n",
    "AGOSHA_LETTERS = [\n",
    "    'ක්', 'ඛ්',\n",
    "    'ච්', 'ඡ්',\n",
    "    'ට්', 'ඨ්',\n",
    "    'ත්', 'ථ්',\n",
    "    'ප්', 'ඵ්',\n",
    "]\n",
    "\n",
    "AGOSHA_TO_GOSHA_MAPPING = {\n",
    "    'ක්': 'ග්',\n",
    "    'ඛ්': 'ඝ්',\n",
    "    'ච්': 'ජ්',\n",
    "    'ඡ්': 'ඣ්',\n",
    "    'ට්': 'ඩ්',\n",
    "    'ඨ්': 'ඪ්',\n",
    "    'ත්': 'ද්',\n",
    "    'ථ්': 'ධ්',\n",
    "    'ප්': 'බ්',\n",
    "    'ඵ්': 'භ්',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "640b6c5d-d2a7-4e7c-b734-481e732052df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sinhala_text = sinhala_text.replace(\"\\u200d\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ebf8e3bc-265b-4516-8872-07bf689813de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'සෞඛ්ය වැඩවර්ජනය අද (02) දිනයේත් අඛණ්ඩව ක්රියාත්මක කරන බව වෘත්තීය සමිති පවසයි.'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sinhala_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "29980126-be28-468f-b66a-3b998feaa0ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ALL_LETTERS = VOWELS + BASE_CONSONANTS + AGOSHA_LETTERS + GOSHA_LETTERS\n",
    "ALL_LETTERS = set(ALL_LETTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "5edd4c12-00f9-472e-8ff9-c3f487d5e976",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PUNKT = set(punctuation)\n",
    "NUMBERS = set(\"1234567890\")\n",
    "\n",
    "NUBERS_AND_PUNKTS = PUNKT.union(NUMBERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "c5dcea48-542c-4bfe-8808-10eec66f4341",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# word = ''\n",
    "# tokenized_chars = []\n",
    "\n",
    "# for i,char in enumerate(sinhala_text):\n",
    "#     if char in VOWEL_DIACRITICS:\n",
    "#         print(f\"{char} skipped\")\n",
    "#         continue\n",
    "#     if char in NUBERS_AND_PUNKTS:\n",
    "#         tokenized_chars.append(char)\n",
    "#     if char == ' ':\n",
    "#         tokenized_chars.append(\" \")\n",
    "#     if (char in ALL_WORDS) and (sinhala_text[i+1] in ALL_WORDS):\n",
    "#         tokenized_chars.append(char)\n",
    "#     if (char in ALL_WORDS) and (sinhala_text[i+1] in VOWEL_DIACRITICS):\n",
    "#         tokenized_chars.append(char + sinhala_text[i+1])\n",
    "#     if (char in ALL_WORDS) and (sinhala_text[i+1] == ' '):\n",
    "#         tokenized_chars.append(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "5454ef5c-7bd9-4533-84be-45e8c9f95ae6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "ගතවු පැය 24 තුළ දිවයින පුරා සිදුකළ මෙහෙයුම්වලින් සැකකරුවන් 703 දෙනෙකු අත්අඩංගුවට ගෙන තිබේ.\n",
    "\n",
    "පොලීසිය පැවසුවේ මත්ද්‍රව්‍ය වැරදි සම්බන්ධ සැකකරුවන් 525ක් සහ අපරාධ අංශ වෙත යොමු කළ ලැයිස්තුවේ සිටි සැකකරුවන් 178ක් ඒ අතර වන බවය.\n",
    "\n",
    "මත්ද්‍රව්‍ය වැරදි සම්බන්ධ අත්අඩංගුවට ගත් සැකකරුවන් 525 දෙනා අතරින් සැකකරුවකු රැඳවුම් නියෝග මත වැඩිදුර විමර්ශන සිදුකරන අතර මත්ද්‍රව්‍යවලට ඇබ්බැහිවූවන් 06 ක් පුනරුත්ථාපනය සඳහා යොමුකර ඇත.\n",
    "\n",
    "පොලිස් මත්ද්‍රව්‍ය නාශක කාර්යංශය සහ පොලිස් විශේෂ කාර්යංශයේ ලැයිස්තුවේ සිටි සැකකරුවන් 04 ක්ද අත්අඩංගුවට ගෙන තිබේ.\n",
    "\n",
    "අපරාධ අංශවෙත යොමු කළ ලැයිස්තුවේ අත්අඩංගුවට ගත් සැකකරුවන් 178 දෙනා අතරින් මත්ද්‍රව්‍ය වැරදි සම්බන්ධව විවෘත වරෙන්තු නිකුත්ව සිටි සැකකරුවන් 11 ක් සහ මත්ද්‍රව්‍ය නොවන වැරදි සම්බන්ධව විවෘත වරෙන්තුකරුවන් 156 ක්ද සිටින බව පොලීසිය පැවසීය.\n",
    "\n",
    "ඇඟිලි සටහන් මාධ්‍යයෙන් හඳුනාගෙන අත්අඩංගුවට ගෙන නොමැතිව සිටි සැකකරුවන් දෙදෙනෙක් සහ අපරාධවලට අවශ්‍ය කරන සැකකරුවන් 09 ක්ද මෙහෙයුම්වලදී අත්අඩංගුවට ගෙන තිබේ.\n",
    "\n",
    "මෙම වැටලීම්වලදී පහත පරිදි මත්ද්‍රව්‍ය පොලිස් භාරයට ගෙන ඇත.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "0d281903-e18f-4360-a336-266543545b0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_non_printable(input_string):\n",
    "    printable_pattern = re.compile(r'[^\\u0020-\\u007E\\u0D80-\\u0DFF]+', flags=re.UNICODE)\n",
    "    return printable_pattern.sub('', input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "1b2e62a3-f9c3-49bc-929c-fb9abb851c5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chracter_level_tokenizer(text):\n",
    "    text = remove_non_printable(text)\n",
    "    tokenized_chars = []\n",
    "\n",
    "    for i, char in enumerate(text):\n",
    "        if char in VOWEL_DIACRITICS:\n",
    "            continue\n",
    "\n",
    "        if char in NUBERS_AND_PUNKTS:\n",
    "            tokenized_chars.append(char)\n",
    "        elif char == ' ':\n",
    "            tokenized_chars.append(\" \")\n",
    "        elif char in ALL_LETTERS:\n",
    "            if i < len(text) - 1 and text[i + 1] in ALL_LETTERS:\n",
    "                tokenized_chars.append(char)\n",
    "            elif i < len(text) - 1 and text[i + 1] in VOWEL_DIACRITICS:\n",
    "                tokenized_chars.append(char + text[i + 1])\n",
    "            else:\n",
    "                tokenized_chars.append(char)\n",
    "        else:\n",
    "            tokenized_chars.append(char)\n",
    "    return tokenized_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "9533581a-ac31-4735-9b90-a69fb8fc48eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"Ransaka/sinhala-450M-sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "69bcba42-9414-425e-9848-2edb021d8765",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.unknown_token = \"<unk>\"\n",
    "        self.tokenized_chars = []\n",
    "        self.unique_chars = []\n",
    "    \n",
    "    def __encode(self, text):\n",
    "        processed_text = self.__process_text(text)\n",
    "        encoded_text = [self.token_map_reverse.get(char, self.unknown_token_id) for char in processed_text]\n",
    "        return encoded_text\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        return self.__encode(text)\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        return [self.token_map.get(token,self.unknown_token) for token in ids]\n",
    "\n",
    "    def train(self, text_list):\n",
    "        self.__train_chracter_level_tokenizer(text_list)\n",
    "\n",
    "    def __process_text(self, t):\n",
    "        t = remove_non_printable(t)\n",
    "        tokenized_chars = []\n",
    "\n",
    "        for i, char in enumerate(t):\n",
    "            if char in VOWEL_DIACRITICS:\n",
    "                continue\n",
    "            if char in NUBERS_AND_PUNKTS:\n",
    "                tokenized_chars.append(char)\n",
    "            elif char == ' ':\n",
    "                tokenized_chars.append(\" \")\n",
    "            elif char in ALL_LETTERS:\n",
    "                if i < len(t) - 1 and t[i + 1] in ALL_LETTERS:\n",
    "                    tokenized_chars.append(char)\n",
    "                elif i < len(t) - 1 and t[i + 1] in VOWEL_DIACRITICS:\n",
    "                    tokenized_chars.append(char + t[i + 1])\n",
    "                else:\n",
    "                    tokenized_chars.append(char)\n",
    "            else:\n",
    "                tokenized_chars.append(char)\n",
    "\n",
    "        return tokenized_chars\n",
    "\n",
    "    def __train_chracter_level_tokenizer(self, text_list):\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            results = list(executor.map(self.__process_text, text_list))\n",
    "            self.tokenized_chars = [char for sublist in results for char in sublist]\n",
    "        self.unique_chars = set(self.tokenized_chars)\n",
    "        self.token_map = dict(zip(range(len(self.unique_chars)),self.unique_chars))\n",
    "        self.token_map[self.unknown_token] = len(self.token_map)\n",
    "        self.unknown_token_id = self.token_map[self.unknown_token]\n",
    "        self.token_map_reverse = {value:key for key,value in self.token_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "708b92de-2b20-4efc-a636-05888731ec9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.train(dataset['train']['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "3f0f4654-f31b-427c-8c33-b11db27ee008",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encode = tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "9bfe25ff-26c3-4892-a648-9932c4cac08e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encode = tokenizer(text_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "0dec06da-fd68-4a52-8424-8a6dad85567f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"Hey my name is Ransaka, බන්ධ අත්අඩංගුවට ගත් සැකකරුවන් 525 දෙනා අතරින් සැකකරුවකු රැඳවුම් may be bbe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "7b0e9589-54be-4a45-8c12-5360de875624",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoded = tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "8dad9ebc-a2c8-414a-9934-4832f9c24725",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sinhala_piece = [tok for tok in encoded if tok != tokenizer.unknown_token_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "8f9115fe-7271-4978-aa1f-effadd5e2d11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_english_characters(text):\n",
    "    \"\"\"\n",
    "    Remove English characters from the given text using a regular expression.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The input text containing English and Sinhala characters.\n",
    "\n",
    "    Returns:\n",
    "    - str: Text with English characters removed.\n",
    "    \"\"\"\n",
    "    english_pattern = re.compile(\"[a-zA-Z]\")\n",
    "    text = english_pattern.sub(r'', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "d1ae6cea-01d1-425a-b4a7-d7d99b035dff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "', බන්ධ අත්අඩංගුවට ගත් සැකකරුවන් 525 දෙනා අතරින් සැකකරුවකු රැඳවුම්'"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_english_characters(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef52b8a0-7869-4bca-9c84-f57baa7ce1ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
